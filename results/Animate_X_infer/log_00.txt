[2025-08-12 20:25:47,043] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 8, 'prefetch_factor': 2, 'resolution': [512, 768], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'save_fps': 8, 'frame_lens': [32, 32, 32, 1], 'sample_fps': [4], 'vid_dataset': {'type': 'VideoBaseDataset', 'data_list': [], 'max_words': 1000, 'resolution': [448, 256]}, 'img_dataset': {'type': 'ImageBaseDataset', 'data_list': ['laion_400m'], 'max_words': 1000, 'resolution': [448, 256]}, 'batch_sizes': {'1': 256, '4': 4, '8': 4, '16': 4}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'linear_sd', 'schedule_param': {'num_timesteps': 1000, 'init_beta': 0.00085, 'last_beta': 0.012, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 30, 'use_div_loss': False, 'p_zero': 0.9, 'guide_scale': 2.5, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_Animate_X', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'config': 'None', 'num': 0, 'no_hand': True, 'num_tokens': 4}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'checkpoints/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'checkpoints/open_clip_pytorch_model.bin'}, 'ema_decay': 0.9999, 'num_steps': 600000, 'lr': 5e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 1000, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': False, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.2, 'resume_checkpoint': 'models/jiuniu_0267000.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 1000, 'resume_checkpoint': '', 'visual_train': {'type': 'VisualTrainTextImageToVideo'}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 100, 'log_dir': 'results/Animate_X_infer', 'seed': 13, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'max_frames': 32, 'round': 5, 'test_list_path': [[1, 'data/images/0001_resized.jpg', 'data/saved_pose/chanel-sample', 'data/saved_frames/chanel', 'data/saved_pkl/chanel.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/humanvid-sample', 'data/saved_frames/humanvid', 'data/saved_pkl/humanvid.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/solodance-sample', 'data/saved_frames/solodance', 'data/saved_pkl/solodance.pkl', 13], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/tiktok-sample', 'data/saved_frames/tiktok', 'data/saved_pkl/tiktok.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/ubc_fashion-sample', 'data/saved_frames/ubc_fashion', 'data/saved_pkl/ubc_fashion.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/vivid-sample', 'data/saved_frames/vivid', 'data/saved_pkl/vivid.pkl', 14]], 'test_model': 'checkpoints/animate-x_ckpt.pth', 'partial_keys': [['image', 'local_image', 'dwpose', 'pose_embeddings']], 'TASK_TYPE': 'inference_animate_x_entrance', 'batch_size': 1, 'latent_random_ref': True, 'scale': 8, 'use_fps_condition': False, 'video_compositions': ['image', 'local_image', 'dwpose', 'randomref', 'randomref_pose', 'pose_embedding'], 'use_DiffusionDPM': False, 'CPU_CLIP_VAE': True, 'cfg_file': 'configs/Animate_X_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': [], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'gpu': 0, 'rank': 0, 'log_file': 'results/Animate_X_infer/log_00.txt'}
[2025-08-12 20:25:47,043] INFO: Running Animate-X inference on gpu 0
[2025-08-12 20:25:47,049] INFO: Loaded ViT-H-14 model config.
[2025-08-12 20:25:51,896] INFO: Loading pretrained ViT-H-14 weights (checkpoints/open_clip_pytorch_model.bin).
[2025-08-12 20:27:50,787] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 8, 'prefetch_factor': 2, 'resolution': [512, 768], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'save_fps': 8, 'frame_lens': [32, 32, 32, 1], 'sample_fps': [4], 'vid_dataset': {'type': 'VideoBaseDataset', 'data_list': [], 'max_words': 1000, 'resolution': [448, 256]}, 'img_dataset': {'type': 'ImageBaseDataset', 'data_list': ['laion_400m'], 'max_words': 1000, 'resolution': [448, 256]}, 'batch_sizes': {'1': 256, '4': 4, '8': 4, '16': 4}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'linear_sd', 'schedule_param': {'num_timesteps': 1000, 'init_beta': 0.00085, 'last_beta': 0.012, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 30, 'use_div_loss': False, 'p_zero': 0.9, 'guide_scale': 2.5, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_Animate_X', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'config': 'None', 'num': 0, 'no_hand': True, 'num_tokens': 4}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'checkpoints/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'checkpoints/open_clip_pytorch_model.bin'}, 'ema_decay': 0.9999, 'num_steps': 600000, 'lr': 5e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 1000, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': False, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.2, 'resume_checkpoint': 'models/jiuniu_0267000.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 1000, 'resume_checkpoint': '', 'visual_train': {'type': 'VisualTrainTextImageToVideo'}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 100, 'log_dir': 'results/Animate_X_infer', 'seed': 13, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'max_frames': 32, 'round': 5, 'test_list_path': [[1, 'data/images/0001_resized.jpg', 'data/saved_pose/chanel-sample', 'data/saved_frames/chanel', 'data/saved_pkl/chanel.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/humanvid-sample', 'data/saved_frames/humanvid', 'data/saved_pkl/humanvid.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/solodance-sample', 'data/saved_frames/solodance', 'data/saved_pkl/solodance.pkl', 13], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/tiktok-sample', 'data/saved_frames/tiktok', 'data/saved_pkl/tiktok.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/ubc_fashion-sample', 'data/saved_frames/ubc_fashion', 'data/saved_pkl/ubc_fashion.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/vivid-sample', 'data/saved_frames/vivid', 'data/saved_pkl/vivid.pkl', 14]], 'test_model': 'checkpoints/animate-x_ckpt.pth', 'partial_keys': [['image', 'local_image', 'dwpose', 'pose_embeddings']], 'TASK_TYPE': 'inference_animate_x_entrance', 'batch_size': 1, 'latent_random_ref': True, 'scale': 8, 'use_fps_condition': False, 'video_compositions': ['image', 'local_image', 'dwpose', 'randomref', 'randomref_pose', 'pose_embedding'], 'use_DiffusionDPM': False, 'CPU_CLIP_VAE': True, 'cfg_file': 'configs/Animate_X_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': [], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'gpu': 0, 'rank': 0, 'log_file': 'results/Animate_X_infer/log_00.txt'}
[2025-08-12 20:27:50,787] INFO: Running Animate-X inference on gpu 0
[2025-08-12 20:27:50,793] INFO: Loaded ViT-H-14 model config.
[2025-08-12 20:27:55,636] INFO: Loading pretrained ViT-H-14 weights (checkpoints/open_clip_pytorch_model.bin).
[2025-08-12 20:28:01,615] INFO: Restored from checkpoints/v2-1_512-ema-pruned.ckpt
[2025-08-12 20:28:11,173] INFO: Load model from checkpoints/animate-x_ckpt.pth with status <All keys matched successfully>
[2025-08-12 20:28:13,846] INFO: There are 6 videos. with 5 times
[2025-08-12 20:28:13,847] INFO: [0]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/chanel-sample init seed 14 ...
[2025-08-12 20:28:44,252] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 8, 'prefetch_factor': 2, 'resolution': [512, 768], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'save_fps': 8, 'frame_lens': [32, 32, 32, 1], 'sample_fps': [4], 'vid_dataset': {'type': 'VideoBaseDataset', 'data_list': [], 'max_words': 1000, 'resolution': [448, 256]}, 'img_dataset': {'type': 'ImageBaseDataset', 'data_list': ['laion_400m'], 'max_words': 1000, 'resolution': [448, 256]}, 'batch_sizes': {'1': 256, '4': 4, '8': 4, '16': 4}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'linear_sd', 'schedule_param': {'num_timesteps': 1000, 'init_beta': 0.00085, 'last_beta': 0.012, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 30, 'use_div_loss': False, 'p_zero': 0.9, 'guide_scale': 2.5, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_Animate_X', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'config': 'None', 'num': 0, 'no_hand': True, 'num_tokens': 4}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'checkpoints/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'checkpoints/open_clip_pytorch_model.bin'}, 'ema_decay': 0.9999, 'num_steps': 600000, 'lr': 5e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 1000, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': False, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.2, 'resume_checkpoint': 'models/jiuniu_0267000.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 1000, 'resume_checkpoint': '', 'visual_train': {'type': 'VisualTrainTextImageToVideo'}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 100, 'log_dir': 'results/Animate_X_infer', 'seed': 13, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'max_frames': 32, 'round': 5, 'test_list_path': [[1, 'data/images/0001_resized.jpg', 'data/saved_pose/chanel-sample', 'data/saved_frames/chanel-sample', 'data/saved_pkl/chanel-sample.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/humanvid-sample', 'data/saved_frames/humanvid-sample', 'data/saved_pkl/humanvid-sample.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/solodance-sample', 'data/saved_frames/solodance-sample', 'data/saved_pkl/solodance-sample.pkl', 13], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/tiktok-sample', 'data/saved_frames/tiktok-sample', 'data/saved_pkl/tiktok-sample.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/ubc_fashion-sample', 'data/saved_frames/ubc_fashion-sample', 'data/saved_pkl/ubc_fashion-sample.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/vivid-sample', 'data/saved_frames/vivid-sample', 'data/saved_pkl/vivid-sample.pkl', 14]], 'test_model': 'checkpoints/animate-x_ckpt.pth', 'partial_keys': [['image', 'local_image', 'dwpose', 'pose_embeddings']], 'TASK_TYPE': 'inference_animate_x_entrance', 'batch_size': 1, 'latent_random_ref': True, 'scale': 8, 'use_fps_condition': False, 'video_compositions': ['image', 'local_image', 'dwpose', 'randomref', 'randomref_pose', 'pose_embedding'], 'use_DiffusionDPM': False, 'CPU_CLIP_VAE': True, 'cfg_file': 'configs/Animate_X_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': [], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'gpu': 0, 'rank': 0, 'log_file': 'results/Animate_X_infer/log_00.txt'}
[2025-08-12 20:28:44,252] INFO: Running Animate-X inference on gpu 0
[2025-08-12 20:28:44,258] INFO: Loaded ViT-H-14 model config.
[2025-08-12 20:28:49,080] INFO: Loading pretrained ViT-H-14 weights (checkpoints/open_clip_pytorch_model.bin).
[2025-08-12 20:28:54,766] INFO: Restored from checkpoints/v2-1_512-ema-pruned.ckpt
[2025-08-12 20:29:04,322] INFO: Load model from checkpoints/animate-x_ckpt.pth with status <All keys matched successfully>
[2025-08-12 20:29:07,011] INFO: There are 6 videos. with 5 times
[2025-08-12 20:29:07,011] INFO: [0]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/chanel-sample init seed 14 ...
[2025-08-12 20:29:08,595] INFO: Current seed 14 ...
[2025-08-12 20:37:14,395] INFO: {'__name__': 'Config: VideoLDM Decoder', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'max_words': 1000, 'num_workers': 8, 'prefetch_factor': 2, 'resolution': [512, 768], 'vit_out_dim': 1024, 'vit_resolution': [224, 224], 'depth_clamp': 10.0, 'misc_size': 384, 'depth_std': 20.0, 'save_fps': 8, 'frame_lens': [32, 32, 32, 1], 'sample_fps': [4], 'vid_dataset': {'type': 'VideoBaseDataset', 'data_list': [], 'max_words': 1000, 'resolution': [448, 256]}, 'img_dataset': {'type': 'ImageBaseDataset', 'data_list': ['laion_400m'], 'max_words': 1000, 'resolution': [448, 256]}, 'batch_sizes': {'1': 256, '4': 4, '8': 4, '16': 4}, 'Diffusion': {'type': 'DiffusionDDIM', 'schedule': 'linear_sd', 'schedule_param': {'num_timesteps': 1000, 'init_beta': 0.00085, 'last_beta': 0.012, 'zero_terminal_snr': True}, 'mean_type': 'v', 'loss_type': 'mse', 'var_type': 'fixed_small', 'rescale_timesteps': False, 'noise_strength': 0.1, 'ddim_timesteps': 50}, 'ddim_timesteps': 30, 'use_div_loss': False, 'p_zero': 0.9, 'guide_scale': 2.5, 'vit_mean': [0.48145466, 0.4578275, 0.40821073], 'vit_std': [0.26862954, 0.26130258, 0.27577711], 'sketch_mean': [0.485, 0.456, 0.406], 'sketch_std': [0.229, 0.224, 0.225], 'hist_sigma': 10.0, 'scale_factor': 0.18215, 'use_checkpoint': True, 'use_sharded_ddp': False, 'use_fsdp': False, 'use_fp16': True, 'temporal_attention': True, 'UNet': {'type': 'UNetSD_Animate_X', 'in_dim': 4, 'dim': 320, 'y_dim': 1024, 'context_dim': 1024, 'out_dim': 4, 'dim_mult': [1, 2, 4, 4], 'num_heads': 8, 'head_dim': 64, 'num_res_blocks': 2, 'attn_scales': [1.0, 0.5, 0.25], 'dropout': 0.1, 'temporal_attention': True, 'temporal_attn_times': 1, 'use_checkpoint': True, 'use_fps_condition': False, 'use_sim_mask': False, 'config': 'None', 'num': 0, 'no_hand': True, 'num_tokens': 4}, 'guidances': [], 'auto_encoder': {'type': 'AutoencoderKL', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0, 'video_kernel_size': [3, 1, 1]}, 'embed_dim': 4, 'pretrained': 'checkpoints/v2-1_512-ema-pruned.ckpt'}, 'embedder': {'type': 'FrozenOpenCLIPTextVisualEmbedder', 'layer': 'penultimate', 'pretrained': 'checkpoints/open_clip_pytorch_model.bin'}, 'ema_decay': 0.9999, 'num_steps': 600000, 'lr': 5e-05, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'chunk_size': 2, 'decoder_bs': 2, 'alpha': 0.7, 'save_ckp_interval': 1000, 'warmup_steps': 10, 'decay_mode': 'cosine', 'use_ema': False, 'load_from': None, 'Pretrain': {'type': 'pretrain_specific_strategies', 'fix_weight': False, 'grad_scale': 0.2, 'resume_checkpoint': 'models/jiuniu_0267000.pth', 'sd_keys_path': 'models/stable_diffusion_image_key_temporal_attention_x1.json'}, 'viz_interval': 1000, 'resume_checkpoint': '', 'visual_train': {'type': 'VisualTrainTextImageToVideo'}, 'visual_inference': {'type': 'VisualGeneratedVideos'}, 'inference_list_path': '', 'log_interval': 100, 'log_dir': 'results/Animate_X_infer', 'seed': 13, 'negative_prompt': 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms', 'max_frames': 32, 'round': 5, 'test_list_path': [[1, 'data/images/0001_resized.jpg', 'data/saved_pose/chanel-sample', 'data/saved_frames/chanel-sample', 'data/saved_pkl/chanel-sample.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/humanvid-sample', 'data/saved_frames/humanvid-sample', 'data/saved_pkl/humanvid-sample.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/solodance-sample', 'data/saved_frames/solodance-sample', 'data/saved_pkl/solodance-sample.pkl', 13], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/tiktok-sample', 'data/saved_frames/tiktok-sample', 'data/saved_pkl/tiktok-sample.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/ubc_fashion-sample', 'data/saved_frames/ubc_fashion-sample', 'data/saved_pkl/ubc_fashion-sample.pkl', 14], [1, 'data/images/0001_resized.jpg', 'data/saved_pose/vivid-sample', 'data/saved_frames/vivid-sample', 'data/saved_pkl/vivid-sample.pkl', 14]], 'test_model': 'checkpoints/animate-x_ckpt.pth', 'partial_keys': [['image', 'local_image', 'dwpose', 'pose_embeddings']], 'TASK_TYPE': 'inference_animate_x_entrance', 'batch_size': 1, 'latent_random_ref': True, 'scale': 8, 'use_fps_condition': False, 'video_compositions': ['image', 'local_image', 'dwpose', 'randomref', 'randomref_pose', 'pose_embedding'], 'use_DiffusionDPM': False, 'CPU_CLIP_VAE': True, 'cfg_file': 'configs/Animate_X_infer.yaml', 'init_method': 'tcp://localhost:9999', 'debug': False, 'opts': [], 'pmi_rank': 0, 'pmi_world_size': 1, 'gpus_per_machine': 1, 'world_size': 1, 'gpu': 0, 'rank': 0, 'log_file': 'results/Animate_X_infer/log_00.txt'}
[2025-08-12 20:37:14,395] INFO: Running Animate-X inference on gpu 0
[2025-08-12 20:37:14,401] INFO: Loaded ViT-H-14 model config.
[2025-08-12 20:37:19,255] INFO: Loading pretrained ViT-H-14 weights (checkpoints/open_clip_pytorch_model.bin).
[2025-08-12 20:37:24,789] INFO: Restored from checkpoints/v2-1_512-ema-pruned.ckpt
[2025-08-12 20:37:34,233] INFO: Load model from checkpoints/animate-x_ckpt.pth with status <All keys matched successfully>
[2025-08-12 20:37:36,886] INFO: There are 6 videos. with 5 times
[2025-08-12 20:37:36,886] INFO: [0]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/chanel-sample init seed 14 ...
[2025-08-12 20:37:38,506] INFO: Current seed 14 ...
[2025-08-12 20:40:15,964] INFO: video saved in results/Animate_X_infer/0001_resized_chanelsample_seed_14_rank_01_00_00_768x512.mp4!
[2025-08-12 20:40:15,965] INFO: [1]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/humanvid-sample init seed 14 ...
[2025-08-12 20:40:20,268] INFO: Current seed 14 ...
[2025-08-12 20:43:00,042] INFO: video saved in results/Animate_X_infer/0001_resized_humanvidsample_seed_14_rank_01_00_01_768x512.mp4!
[2025-08-12 20:43:00,042] INFO: [2]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/solodance-sample init seed 13 ...
[2025-08-12 20:43:02,953] INFO: Current seed 13 ...
[2025-08-12 20:45:42,959] INFO: video saved in results/Animate_X_infer/0001_resized_solodancesample_seed_13_rank_01_00_02_768x512.mp4!
[2025-08-12 20:45:42,959] INFO: [3]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/tiktok-sample init seed 14 ...
[2025-08-12 20:45:44,653] INFO: Current seed 14 ...
[2025-08-12 20:48:24,627] INFO: video saved in results/Animate_X_infer/0001_resized_tiktoksample_seed_14_rank_01_00_03_768x512.mp4!
[2025-08-12 20:48:24,627] INFO: [4]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/ubc_fashion-sample init seed 14 ...
[2025-08-12 20:48:27,534] INFO: Current seed 14 ...
[2025-08-12 20:51:07,495] INFO: video saved in results/Animate_X_infer/0001_resized_ubc_fashionsample_seed_14_rank_01_00_04_768x512.mp4!
[2025-08-12 20:51:07,495] INFO: [5]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/vivid-sample init seed 14 ...
[2025-08-12 20:51:09,016] INFO: Current seed 14 ...
[2025-08-12 20:53:49,051] INFO: video saved in results/Animate_X_infer/0001_resized_vividsample_seed_14_rank_01_00_05_768x512.mp4!
[2025-08-12 20:53:49,051] INFO: [6]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/chanel-sample init seed 15 ...
[2025-08-12 20:53:50,646] INFO: Current seed 15 ...
[2025-08-12 20:56:30,745] INFO: video saved in results/Animate_X_infer/0001_resized_chanelsample_seed_15_rank_01_00_06_768x512.mp4!
[2025-08-12 20:56:30,745] INFO: [7]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/humanvid-sample init seed 15 ...
[2025-08-12 20:56:35,302] INFO: Current seed 15 ...
[2025-08-12 20:59:15,479] INFO: video saved in results/Animate_X_infer/0001_resized_humanvidsample_seed_15_rank_01_00_07_768x512.mp4!
[2025-08-12 20:59:15,480] INFO: [8]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/solodance-sample init seed 14 ...
[2025-08-12 20:59:18,380] INFO: Current seed 14 ...
[2025-08-12 21:01:58,412] INFO: video saved in results/Animate_X_infer/0001_resized_solodancesample_seed_14_rank_01_00_08_768x512.mp4!
[2025-08-12 21:01:58,413] INFO: [9]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/tiktok-sample init seed 15 ...
[2025-08-12 21:02:00,088] INFO: Current seed 15 ...
[2025-08-12 21:04:40,150] INFO: video saved in results/Animate_X_infer/0001_resized_tiktoksample_seed_15_rank_01_00_09_768x512.mp4!
[2025-08-12 21:04:40,150] INFO: [10]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/ubc_fashion-sample init seed 15 ...
[2025-08-12 21:04:42,993] INFO: Current seed 15 ...
[2025-08-12 21:07:22,939] INFO: video saved in results/Animate_X_infer/0001_resized_ubc_fashionsample_seed_15_rank_01_00_10_768x512.mp4!
[2025-08-12 21:07:22,939] INFO: [11]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/vivid-sample init seed 15 ...
[2025-08-12 21:07:24,455] INFO: Current seed 15 ...
[2025-08-12 21:10:04,512] INFO: video saved in results/Animate_X_infer/0001_resized_vividsample_seed_15_rank_01_00_11_768x512.mp4!
[2025-08-12 21:10:04,512] INFO: [12]/[30] Begin to sample data/images/0001_resized.jpg, pose sequence from data/saved_pose/chanel-sample init seed 16 ...
[2025-08-12 21:10:06,138] INFO: Current seed 16 ...
